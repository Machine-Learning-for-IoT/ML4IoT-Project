{"cells":[{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704358983165,"execution_millis":2023,"deepnote_to_be_reexecuted":false,"cell_id":"4927511a13a2434f9d54bc8d9cfe213f","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nimport os\nimport numpy as np\nfrom time import time\nimport pandas as pd\nimport zipfile\n\n\nfrom preprocessing import LABELS\nfrom preprocessing import AudioReader\nfrom preprocessing import MelSpectrogram, MFCC","block_group":"4927511a13a2434f9d54bc8d9cfe213f","execution_count":null,"outputs":[{"name":"stderr","text":"2024-01-04 09:03:03.284608: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-01-04 09:03:03.286453: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-01-04 09:03:03.323137: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-01-04 09:03:03.324141: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-01-04 09:03:04.176460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704358985197,"execution_millis":12,"deepnote_to_be_reexecuted":false,"cell_id":"5179af7abb434ed9a572d00f85c28bf5","deepnote_cell_type":"code"},"source":"# Seed value\n# Apparently you may use different seed values at each stage\nseed_value= 2\n\n# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\n# 2. Set the `python` built-in pseudo-random generator at a fixed value\nimport random\nrandom.seed(seed_value)\n\n# 3. Set the `numpy` pseudo-random generator at a fixed value\nnp.random.seed(seed_value)\n\n# 4. Set the `tensorflow` pseudo-random generator at a fixed value\ntf.random.set_seed(seed_value)\n# for later versions: \n# tf.compat.v1.set_random_seed(seed_value)\n\n# 5. Configure a new global `tensorflow` session\n\n# for later versions:\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)","block_group":"d1ec4b7eb15841bf88879ef468038deb","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"4c4387f773b240b193cfb3ddc4779af4","deepnote_cell_type":"text-cell-h2"},"source":"## 1. Defining hyper-parameters","block_group":"3bc7e15ff35c4bfda3b89ec353f89654"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704358985209,"execution_millis":14,"deepnote_to_be_reexecuted":false,"cell_id":"d8bc68e1d4874caeb573c80cbb8e0620","deepnote_cell_type":"code"},"source":"PREPROCESSING_ARGS = { 'sampling_rate': 16000,\n    'frame_length_in_s': 0.032,\n    'frame_step_in_s': 0.016,\n    'num_mel_bins': 16, # Triet 32\n    'lower_frequency': 20,\n    'upper_frequency': 4000,\n    'num_coefficients': 20 # Triet 13\n}\n\nTRAINING_ARGS = {\n    'batch_size': 20,\n    'initial_learning_rate': 0.01,\n    #'end_learning_rate': 1.e-5,\n    'end_learning_rate': 0.001,\n    'epochs': 20\n}","block_group":"bc7262cb7dae4a5ab163965eaf22996e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"15dd6f784bb147aea9aea51947af6736","deepnote_cell_type":"text-cell-h2"},"source":"## 2. Create train / test datasets","block_group":"66e13da1b14b4b37b8e909991c3bdd48"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704358985213,"execution_millis":171,"deepnote_to_be_reexecuted":false,"cell_id":"26b9aedb31ac4adcb7e059d8362f89c9","deepnote_cell_type":"code"},"source":"train_ds = tf.data.Dataset.list_files('/tmp/yn-train/*')\ntest_ds = tf.data.Dataset.list_files('/tmp/yn-test/*')","block_group":"c61348100cda43fcab860340a12661dc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704358985425,"execution_millis":1877,"deepnote_to_be_reexecuted":false,"cell_id":"3675744c17c84fada225ef62ba63e1c5","deepnote_cell_type":"code"},"source":"audio_reader = AudioReader(tf.int16, 16000)\nmfcc_spec_processor = MFCC(**PREPROCESSING_ARGS)\n\ndef prepare_for_training(feature, label):\n    feature = tf.expand_dims(feature, -1)\n    label_id = tf.argmax(label == LABELS)\n\n    return feature, label_id\n\n\nbatch_size = TRAINING_ARGS['batch_size']\nepochs = TRAINING_ARGS['epochs']\n\ntrain_ds = (train_ds\n            .map(audio_reader.get_audio_and_label)\n            .map(mfcc_spec_processor.get_mfccs_and_label)\n            .map(prepare_for_training)\n            .batch(batch_size)\n            .cache())\n\ntest_ds = (test_ds\n            .map(audio_reader.get_audio_and_label)\n            .map(mfcc_spec_processor.get_mfccs_and_label)\n            .map(prepare_for_training)\n            .batch(batch_size))","block_group":"ad742bd2de684386a8c6e8e751228f85","execution_count":null,"outputs":[{"name":"stderr","text":"2024-01-04 09:03:05.961317: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX AVX2 AVX512F FMA\n2024-01-04 09:03:05.963125: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704358987300,"execution_millis":165,"deepnote_to_be_reexecuted":false,"cell_id":"600e109f79864d4d8e095808e17e96f2","deepnote_cell_type":"code"},"source":"for example_batch, example_labels in train_ds.take(1):\n  print('Batch Shape:', example_batch.shape)\n  print('Data Shape:', example_batch.shape[1:])\n  print('Labels:', example_labels)","block_group":"0d04a6e205b34b96bf1133247a896f7b","execution_count":null,"outputs":[{"name":"stdout","text":"Batch Shape: (20, 61, 16, 1)\nData Shape: (61, 16, 1)\nLabels: tf.Tensor([1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1], shape=(20,), dtype=int64)\n2024-01-04 09:03:07.449379: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"6b4d9216982f452ba5cbf0d3d203e804","deepnote_cell_type":"text-cell-h2"},"source":"## 3. Train model","block_group":"94b95f1d2623429fa9fa7aabc160bd36"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"226f818ef47c4795811e6883b9acbaab","deepnote_cell_type":"text-cell-p"},"source":"workflow: train many models, save models and hyper-parameters. If a model reaches required accuracy, move on to test latency","block_group":"f6fea2ad81eb4e618e5608ac576e4d7f"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"6e29d3b1251e45f68db9c8c6b4efb4d4","deepnote_cell_type":"text-cell-h3"},"source":"### 3.1 Model creation","block_group":"b18f2b5f991549f3bedbd94d813cd3ee"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704358987534,"execution_millis":85,"deepnote_to_be_reexecuted":false,"cell_id":"137806be89ae46c1ac36e5da94ba4f15","deepnote_cell_type":"code"},"source":"#depth-wise convolution\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n    tf.keras.layers.Conv2D(filters=32, kernel_size=[5, 5], strides=[2, 2],\n        use_bias=False, padding='valid'),\n    tf.keras.layers.MaxPool2D(padding=\"same\"),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dropout(0.02),\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[5, 5], strides=[1, 1], \n        use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=32, kernel_size=[1, 1], strides=[1, 1],   \n       use_bias=False),\n    tf.keras.layers.MaxPool2D(padding=\"same\"),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dropout(0.02),\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[5, 5], strides=[1, 1],\n        use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=32, kernel_size=[1, 1], strides=[1, 1],   \n       use_bias=False),\n    tf.keras.layers.MaxPool2D(padding=\"same\"),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dropout(0.02),\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(units=2),\n    tf.keras.layers.Softmax()\n])","block_group":"9b64d0a6bbad49edbfb31d6dacf3770f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704358987621,"execution_millis":624,"deepnote_to_be_reexecuted":false,"cell_id":"0824f4caa2e54f1b893c7a119b626d14","deepnote_cell_type":"code"},"source":"import tensorflow_model_optimization as tfmot\n\nfinal_sparsity = 0.9\nprune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n\nbegin_step = int(len(train_ds) * epochs * 0.1 )\nend_step = int(len(train_ds) * epochs )\n\npruning_params = {\n    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n        initial_sparsity=0.10,\n        final_sparsity=final_sparsity,\n        begin_step=begin_step,\n        end_step=end_step \n    )\n}\n\nmodel_for_pruning = prune_low_magnitude(model, **pruning_params)","block_group":"caefd256f1694dd6a8714e2b8d30595e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c08d2fca41bc47c1b633f48cc0d85395","deepnote_cell_type":"text-cell-h3"},"source":"### 3.2 Training","block_group":"151a18bd969342898da49adaf834cff6"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704358988247,"execution_millis":27090,"deepnote_to_be_reexecuted":false,"cell_id":"54fef4851d0a41eb9aa311e7d0544297","deepnote_cell_type":"code"},"source":"loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\ninitial_learning_rate = TRAINING_ARGS['initial_learning_rate']\nend_learning_rate = TRAINING_ARGS['end_learning_rate']\n\nlinear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=initial_learning_rate,\n    end_learning_rate=end_learning_rate,\n    decay_steps=len(train_ds) * epochs,\n)\noptimizer = tf.optimizers.Adam(learning_rate=linear_decay)\nmetrics = [tf.metrics.SparseCategoricalAccuracy()]\ncallbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\nmodel_for_pruning.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\nhistory = model_for_pruning.fit(train_ds, epochs=epochs, validation_data=test_ds, callbacks=callbacks)\n","block_group":"0d7d5abc418f49d7b3b6797635e021ef","execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/20\n80/80 [==============================] - 7s 49ms/step - loss: 0.2443 - sparse_categorical_accuracy: 0.8975 - val_loss: 0.1759 - val_sparse_categorical_accuracy: 0.9500\nEpoch 2/20\n80/80 [==============================] - 1s 15ms/step - loss: 0.0963 - sparse_categorical_accuracy: 0.9681 - val_loss: 0.1232 - val_sparse_categorical_accuracy: 0.9550\nEpoch 3/20\n80/80 [==============================] - 1s 13ms/step - loss: 0.0871 - sparse_categorical_accuracy: 0.9688 - val_loss: 0.0467 - val_sparse_categorical_accuracy: 0.9700\nEpoch 4/20\n80/80 [==============================] - 1s 14ms/step - loss: 0.0624 - sparse_categorical_accuracy: 0.9756 - val_loss: 0.0326 - val_sparse_categorical_accuracy: 0.9900\nEpoch 5/20\n80/80 [==============================] - 1s 13ms/step - loss: 0.0479 - sparse_categorical_accuracy: 0.9794 - val_loss: 0.0427 - val_sparse_categorical_accuracy: 0.9850\nEpoch 6/20\n80/80 [==============================] - 1s 13ms/step - loss: 0.0302 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0319 - val_sparse_categorical_accuracy: 0.9900\nEpoch 7/20\n80/80 [==============================] - 1s 13ms/step - loss: 0.0206 - sparse_categorical_accuracy: 0.9931 - val_loss: 0.0418 - val_sparse_categorical_accuracy: 0.9850\nEpoch 8/20\n80/80 [==============================] - 1s 13ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.0730 - val_sparse_categorical_accuracy: 0.9850\nEpoch 9/20\n80/80 [==============================] - 1s 12ms/step - loss: 0.0450 - sparse_categorical_accuracy: 0.9825 - val_loss: 0.0137 - val_sparse_categorical_accuracy: 0.9950\nEpoch 10/20\n80/80 [==============================] - 1s 13ms/step - loss: 0.0345 - sparse_categorical_accuracy: 0.9856 - val_loss: 0.0337 - val_sparse_categorical_accuracy: 0.9850\nEpoch 11/20\n80/80 [==============================] - 1s 13ms/step - loss: 0.0399 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.0334 - val_sparse_categorical_accuracy: 1.0000\nEpoch 12/20\n80/80 [==============================] - 1s 12ms/step - loss: 0.0413 - sparse_categorical_accuracy: 0.9819 - val_loss: 0.0500 - val_sparse_categorical_accuracy: 0.9900\nEpoch 13/20\n80/80 [==============================] - 1s 12ms/step - loss: 0.0437 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.0330 - val_sparse_categorical_accuracy: 0.9950\nEpoch 14/20\n80/80 [==============================] - 1s 11ms/step - loss: 0.0360 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0276 - val_sparse_categorical_accuracy: 0.9950\nEpoch 15/20\n80/80 [==============================] - 1s 12ms/step - loss: 0.0397 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0389 - val_sparse_categorical_accuracy: 0.9850\nEpoch 16/20\n80/80 [==============================] - 1s 11ms/step - loss: 0.0388 - sparse_categorical_accuracy: 0.9869 - val_loss: 0.0437 - val_sparse_categorical_accuracy: 0.9900\nEpoch 17/20\n80/80 [==============================] - 1s 13ms/step - loss: 0.0382 - sparse_categorical_accuracy: 0.9869 - val_loss: 0.0578 - val_sparse_categorical_accuracy: 0.9850\nEpoch 18/20\n80/80 [==============================] - 1s 12ms/step - loss: 0.0510 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.0764 - val_sparse_categorical_accuracy: 0.9800\nEpoch 19/20\n80/80 [==============================] - 1s 13ms/step - loss: 0.0436 - sparse_categorical_accuracy: 0.9850 - val_loss: 0.0440 - val_sparse_categorical_accuracy: 0.9950\nEpoch 20/20\n80/80 [==============================] - 1s 11ms/step - loss: 0.0354 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0382 - val_sparse_categorical_accuracy: 0.9950\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"6cb0a61fecbf4dd9b00fcd1f267928fe","deepnote_cell_type":"text-cell-h3"},"source":"### 3.3 Testing","block_group":"3b00a2897eb24d339aa97fa4b78e2895"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704359015336,"execution_millis":505,"deepnote_to_be_reexecuted":false,"cell_id":"e0a25f45f52847dda0ed6c0baf0bd450","deepnote_cell_type":"code"},"source":"test_loss, test_accuracy = model_for_pruning.evaluate(test_ds)","block_group":"1a1abea3e74e440c9d097d7d934ed578","execution_count":null,"outputs":[{"name":"stdout","text":"10/10 [==============================] - 0s 32ms/step - loss: 0.0382 - sparse_categorical_accuracy: 0.9950\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"ebf5c5946f3a404b88564838f4279a64","deepnote_cell_type":"text-cell-h3"},"source":"### 3.4 Saving model and hyper-parameters","block_group":"ce52981e1a2e4d1db38b4ed70cb41b95"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704359015842,"execution_millis":10565,"deepnote_to_be_reexecuted":false,"cell_id":"176e05387f80464192e53ff8622fe5dc","deepnote_cell_type":"code"},"source":"if test_accuracy > 0.989:\n    #timestamp = int(time())\n    timestamp = seed_value\n\n    #save model\n    saved_model_dir = f'./triet/saved_models/{timestamp}'\n    if not os.path.exists(saved_model_dir):\n        os.makedirs(saved_model_dir)\n    model.save(saved_model_dir)\n    \n    #save parameters\n    output_dict = {\n        'timestamp': timestamp,\n        **PREPROCESSING_ARGS,\n        **TRAINING_ARGS,\n        'test_accuracy': test_accuracy\n    }\n\n    df = pd.DataFrame([output_dict])\n\n    parameter_dir ='./triet/saved_hyperparameters/'\n    if not os.path.exists(parameter_dir):\n        os.makedirs(parameter_dir)\n    output_path = f'./triet/saved_hyperparameters/{timestamp}.csv'\n    df.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)\n\n    #save tflite model\n    tflite_models_dir = './triet/tflite_models'\n    if not os.path.exists(tflite_models_dir):\n        os.makedirs(tflite_models_dir)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n\n    tflite_model_name = os.path.join(tflite_models_dir, f'{timestamp}.tflite')\n\n    with open(tflite_model_name, 'wb') as fp:\n        fp.write(tflite_model)\n    saved_path = tflite_model_name\n    with zipfile.ZipFile(f'{tflite_model_name}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n        f.write(tflite_model_name)\n\n    tflite_size = os.path.getsize(tflite_model_name) / 1024.0\n    zipped_size = os.path.getsize(f'{tflite_model_name}.zip') / 1024.0\n\n    print(f'Original tflite size (pruned model): {tflite_size:.3f} KB')\n    print(f'Zipped tflite size (pruned model): {zipped_size:.3f} KB')\n","block_group":"76ee96157305452fa795db9f7bd24951","execution_count":null,"outputs":[{"name":"stdout","text":"WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./triet/saved_models/2/assets\nINFO:tensorflow:Assets written to: ./triet/saved_models/2/assets\n2024-01-04 09:03:44.280055: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n2024-01-04 09:03:44.280090: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n2024-01-04 09:03:44.445932: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./triet/saved_models/2\n2024-01-04 09:03:44.644760: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n2024-01-04 09:03:44.644791: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: ./triet/saved_models/2\n2024-01-04 09:03:44.648276: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n2024-01-04 09:03:44.649387: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n2024-01-04 09:03:45.052478: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: ./triet/saved_models/2\n2024-01-04 09:03:45.064969: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 619050 microseconds.\n2024-01-04 09:03:45.076894: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\nOriginal tflite size (pruned model): 24.137 KB\nZipped tflite size (pruned model): 11.289 KB\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"ba72a0ea7c224a9ba7689c14048337cb","deepnote_cell_type":"text-cell-h2"},"source":"## 4. Test model","block_group":"afe48a30fbe546adb45cf9b786233742"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"9aebfa124f4948b7ac33b47ce3e01ef7","deepnote_cell_type":"text-cell-h3"},"source":"### 4.1 Define reference model","block_group":"6a5a4d5095324c5f8c9bbd8ea7dac4fc"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704359026408,"execution_millis":150,"deepnote_to_be_reexecuted":false,"cell_id":"59e15b8b32e34a54b9b46b32b1584a09","deepnote_cell_type":"code"},"source":"REF_PREPROCESSING_ARGS = {\n    'sampling_rate': 16000,\n    'frame_length_in_s': 0.04,\n    'frame_step_in_s': 0.02,\n    'num_mel_bins': 40,\n    'lower_frequency': 20,\n    'upper_frequency': 4000,\n}\n\ntflite_models_dir = './tflite_models'\nif not os.path.exists(tflite_models_dir):\n    os.makedirs(tflite_models_dir)\ntflite_model_name = os.path.join(tflite_models_dir, 'ref_model.tflite')\n\nif not os.path.exists(tflite_model_name):\n    ref_model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=[49, 40, 1]),\n        tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[2, 2], use_bias=False, padding='valid'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(units=2),\n        tf.keras.layers.Softmax()\n    ])\n\n    ref_model.build()\n\n    saved_model_dir = f'./saved_models/ref_model'\n    if not os.path.exists(saved_model_dir):\n        os.makedirs(saved_model_dir)\n    ref_model.save(saved_model_dir)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(f'./saved_models/ref_model')\n    tflite_model = converter.convert()\n\n    with open(tflite_model_name, 'wb') as fp:\n        fp.write(tflite_model)\n","block_group":"2dac24f474774344b598db7d35717d0f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"990a3d7a00194d89a9580c0420da3832","deepnote_cell_type":"text-cell-h3"},"source":"### 4.2 Measure Latency Saving","block_group":"e67ba2e39edf4c129d4a561996db693d"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704359026553,"execution_millis":2246,"deepnote_to_be_reexecuted":false,"cell_id":"36e3c4565c1d47f883b93cea5adedda5","deepnote_cell_type":"code"},"source":"mel_spec_processor = MelSpectrogram(**REF_PREPROCESSING_ARGS)\ninterpreter = tf.lite.Interpreter(model_path='tflite_models/ref_model.tflite')\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\naudio = tf.random.normal((16000,))\n\nref_latencies = []\n\nfor i in range(100):\n    start_preprocess = time()\n\n    log_mel_spectrogram = mel_spec_processor.get_mel_spec(audio)\n    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, 0)\n    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)\n    interpreter.set_tensor(input_details[0]['index'], log_mel_spectrogram)\n    interpreter.invoke()\n    output = interpreter.get_tensor(output_details[0]['index'])\n\n    end_inference = time()\n\n    ref_latencies.append(end_inference - start_preprocess)\n\nmedian_ref_latency = np.median(ref_latencies)\n\n####################################################3\nmfcc_spec_processor = MFCC(**PREPROCESSING_ARGS)\n#interpreter = tf.lite.Interpreter(model_path='./triet/tflite_models/1704039608.tflite')\ninterpreter = tf.lite.Interpreter(model_path=saved_path)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\naudio = tf.random.normal((16000,))\n\nlatencies = []\n\nfor i in range(100):\n    start_preprocess = time()\n\n    mfcc = mfcc_spec_processor.get_mfccs(audio)\n    mfcc = tf.expand_dims(mfcc, 0)\n    mfcc = tf.expand_dims(mfcc, -1)\n    interpreter.set_tensor(input_details[0]['index'], mfcc)\n    interpreter.invoke()\n    output = interpreter.get_tensor(output_details[0]['index'])\n\n    end_inference = time()\n\n    latencies.append(end_inference - start_preprocess)\n\noptimized_latency = np.median(latencies)\n\n\nprint(100 * (median_ref_latency - optimized_latency) / median_ref_latency)\n","block_group":"048b8ac5cf0a4765b7318b01792a26b4","execution_count":null,"outputs":[{"name":"stderr","text":"INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n55.5087001450894\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704359028796,"execution_millis":10,"deepnote_to_be_reexecuted":false,"cell_id":"1936e5284ca64f5faf2d627e9ad4d1e1","deepnote_cell_type":"code"},"source":"","block_group":"cce05580337745ada2174a86d4121ba5","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0ec8e575-19c8-4894-9160-01c0b36ec399' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"28f5364d9a28492ea8ce1866d2fbba19","deepnote_execution_queue":[]}}